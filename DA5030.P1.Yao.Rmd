---
title: "Practicum1"
author: "yaochieh yao"
date: "2/9/2024"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r package_management, echo=FALSE, message=FALSE, warning=FALSE}
library(class)
library(ggplot2)
library(knitr)
library(tidyr)
library(kableExtra)
```



## 1 / Predicting Diabetes
```{r EDA, echo=FALSE, message=FALSE, warning=FALSE}
df_db<-read.csv("diabetes.csv")
# Drop ID column
# head(df_db)
# tail(df_db)
# str(df_db)
# summary(df_db)
# Besides the outcome category as zero and 1, in other columns, 0 represents the NA value. 
```



### 1.1 / Analysis of Data Distribution <br>
```{r analyze_distributions, echo=FALSE, message=FALSE, warning=FALSE}
# Calculate mean and sd to draw normal curve
mean_glucose<-mean(df_db$Glucose,na.rm = TRUE)
sd_glucose<-sd(df_db$Glucose,na.rm = TRUE)

# Remove NA
df_db$Glucose[df_db$Glucose == 0] <- NA
Glucose<-na.omit(df_db[,"Glucose"])

# Plot histogram  
ggplot(df_db, aes(x=Glucose)) + 
  geom_histogram(aes(y = ..density..),breaks = seq(0, 250, by = 10), color = "black", fill = "white") +
  stat_function(fun = dnorm, args = list(mean = mean_glucose, sd = sd_glucose), color = "red", size = 0.5) +
  theme_minimal()
```
<br>
The histogram shows Glucose data are in a slightly right screw distribution (tail on the right) than normal, and there are more data higher than its mean value. The visualization gives us an idea of our data, which benefits our imputing strategy and data processing. For example, we will consider using the median instead of the mean when the distribution right-screwed;If the distribution is not normal, then we need to be careful, since it might influence our analysis like linear regression. 

```{r normality_test, echo=FALSE, message=FALSE, warning=FALSE}
# Test normality by Shapiro-Test 
test_result<-shapiro.test(Glucose)
```
The Shapiro-Test is to examine the normality of our data; the resulted p-value is `r test_result$p.value`, less than 0.05, which rejects the normality hypothesis, and we can confirm that Glucose is not in a normal distribution.


### 1.2 / Identification of Outliers
We use the Z-score deviation approach for outlier identification, assuming all variables follow a normal distribution. This approach involves calculating the Z-score for each data point X, measuring the distance from the mean within a standard deviation σ. By assigning an expected standard deviation as our cut-off, we can identify outliers based on their Z-score distance, which is either over or less than our threshold.

Z=(Xi−μ) / σ

For example, assigning σ=3 gives us a 99.7% confidence that our data points within this range are not outliers, and σ =2 will cover 95%. In other words, a higher standard deviation reduces the tolerance of variation and, thus, results in fewer outliers.

Here, we assign standard deviation = 2.5 and get total outliers =104. We break down the outliers of each column as followed table:<br>
<br>
```{r identify_outliers, echo=FALSE, message=FALSE, warning=FALSE,results='asis'}
# calculate only numeric columns
colum_names<-colnames(df_db)[colnames(df_db) != "Outcome"]

# Define an empty list to store outliers
# outlier_list <- list()
outlier_summary <- data.frame(Column = character(), Outliers=I(list()) ,Values = I(list()))


for (col in colum_names){
  # replace 0 with NA
  df_db[,col][df_db[,col] == 0 ] <- NA
  
  # Remove NA
  find_outliers<-na.omit(df_db[,col])
  
  # Calculate mean and sd for each
  mean_col<-mean(find_outliers)
  sd_col<- sd(find_outliers)
  
  # Get Zscore for filtering outliers
  zScore_col <- abs(find_outliers-mean_col) / sd_col
  outliers<- find_outliers[zScore_col>2.5]
  
  # Save outliers in each column
  # outlier_list[[col]]<-outliers
  outlier_summary <- rbind(outlier_summary, data.frame(Column = col, Outliers = I(list(length(outliers))), Values = I(list(outliers)), stringsAsFactors = FALSE))

}

# summary(df_db)
# outlier values saperation
outlier_summary$Values <- sapply(outlier_summary$Values, function(x) paste(x, collapse = ", "))

knitr::kable(outlier_summary, format = "html",caption = "**Outlier Summary**") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

```

However, this method is based on a distribution assumption; checking our data distribution in each column in advance is essential, as we did in 1.1.   


According to observation, the sampling of the diabetes dataset has more elderly than younger age, and those are natural variants, not technical errors or extreme cases. Maybe due to the nature of the disease, there are more elderly than young patients. In addition, in the Glucose, Blood Pressure, Insulin, BMI, and Diabetes Pedigree Function, the outliers are meaningful disease indicators for diabetes; for example, high blood pressure and glucose are disease phenotypes. Finally, a specific diabetes subtype, Gestational diabetes, is associated with hormone changes during pregnancy. Once having suspicious diabetes syndrome,  doctors will 
suggest testing within 12 weeks after the baby is born; this might explain why 13 to 17 are outliers in pregnancies. Overall, since those outliers are meaningful information associated with diabetes, we do not need to process them. <br>


### 1.3 / Data Preparation
```{r data_preparation, message=FALSE, warning=FALSE}
# Standardization function (z-score)
standardize <- function(x) {
  # replace 0 with NA
  x[x == 0 ] <- NA
  # Remove NA
  return((x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE))
}

# Normalize all numeric columns
df_std <- as.data.frame(lapply(df_db[colum_names], standardize))

```
Dataset have different features with different scales, as we can see from the summary. If one feature has a higher influence than others, the result of our prediction becomes biased, especially numerical data. For example, KNN uses distance calculation if one feature has a value over thousands, but the rest of the value is only between 0 and 1; the predicted result will always relate to the dominant feature. Normalization can help to ensure that numerical values are within a reasonable range.<br>
<br>
Here, we are using standardization for normalization. Compared to the max-min normalization force scale of one data range to another, standardization transforms our data to have a mean of 0 and a standard deviation of 1, making it not only scale-independent but also shape-conservative regarding the original distribution, 
which makes it less sensitive to outliers.    


### 1.4 / Sampling Training and Validation Data
```{r data_splitting, message=FALSE, warning=FALSE}
# Select 20% outcome one positive and 20% outcome zero negative. 
positive_outcome <- which(df_db$Outcome == 1)
negative_outcome <- which(df_db$Outcome == 0)

set.seed(123) 
positive_samples<-sample(positive_outcome, round(0.2*(length(positive_outcome))))
negative_samples<- sample(negative_outcome, round(0.2*(length(negative_outcome))))

valid_idx<-c(positive_samples,negative_samples)

valid_data <- df_db[valid_idx,]
train_data <- df_db[-valid_idx,]

train_labels <- train_data$Outcome
valid_labels <- valid_data$Outcome

train_data <- train_data[, !(names(train_data) %in% "Outcome")]
valid_data <- valid_data[, !(names(valid_data) %in% "Outcome")]

# Train Data Processing
for (col in colum_names){
  # replace 0 with NA
  train_data[, col][train_data[, col] == 0] <- NA
  
  # Calculate median for each columns
  median_value <- median(train_data[, col], na.rm = TRUE)
  
  # Imputing NA with median 
  train_data[, col][is.na(train_data[, col])] <- median_value
}

train_data_std <- as.data.frame(lapply(train_data[colum_names], standardize))


# Valid Data Processing
for (col in colum_names){
  # replace 0 with NA
  valid_data[, col][valid_data[, col] == 0] <- NA
  
  # Calculate median for each columns
  median_value <- median(valid_data[, col], na.rm = TRUE)
  
  # Imputing NA with median 
  valid_data[, col][is.na(valid_data[, col])] <- median_value
}

valid_data_std <- as.data.frame(lapply(valid_data[colum_names], standardize))

```


### 1.5 / Predictive Modeling
```{r predictive_modeling, message=FALSE, warning=FALSE}
class_predicted <- knn(train = train_data_std, test = valid_data_std, cl = train_labels, k = 5)
class_result<-table(valid_labels,class_predicted)
class_accuracy <- round(sum(diag(class_result))/length(valid_labels),2)
class_accuracy
```

### 1.6 / Model Accuracy
```{r model_evaluation, echo=FALSE, message=FALSE, warning=FALSE}

accuracies <- setNames(numeric(length = 9), as.numeric(2:10))

set.seed(124) 
for (k in 2:10){
  class_predicted <- knn(train = train_data_std, test = valid_data_std, cl = train_labels, k = k)
  class_result<-table(valid_labels,class_predicted)
  accuracies[as.character(k)] <- round(sum(diag(class_result))/length(valid_labels)*100,2)
}

# plot(names(accuracies),accuracies,col = 'red', xlab = 'K', ylab = 'Accuracy (%)',main = 'K vs. Accuracy', pch = 19)
# Convert to dataframe for visualization
accuracies_df <- data.frame(
  K = as.numeric(names(accuracies)), 
  Accuracy = accuracies
)

ggplot(accuracies_df, aes(x = K, y = Accuracy)) +
  geom_point(color = 'red') + 
  geom_line(color = 'red', group = 1) +  
  labs(x = 'K', y = 'Accuracy (%)') +
  theme_minimal()

optimal_k<-names(which.max(accuracies))
print(paste0("The optimal k is ",optimal_k," with accuracy ",accuracies[optimal_k],"%"))

```
**CONCLUSION** <br>
First, we randomly created a stratified sample in the data processing pipeline by
selecting 20% positive "Outcome" and another 20% negative for validation. Then, 
those are split as valid datasets and the non-selected data for training. After 
proper separation, I continue with NA value inputting using the median value, then 
perform normalization using the standardization method. The "K vs. Accuracy" plot 
indicates a positive correlation; when k increases, prediction accuracy is also 
enhanced.However, in k=9 , the accuracy reach up the peak and slight drop when 
k=10. Therefore, we conclude base on the plot, the optimal k = `r optimal_k` , 
and provides the accuracy in `r accuracies[optimal_k]`%.
<br>
<br>
<br>

## 2 / Predicting Age of Abalones using Regression kNN

### 2.1 / Data 
```{r split_dataset,echo=FALSE, message=FALSE, warning=FALSE}
df_abl<-read.csv("abalone.csv")
target_data<-df_abl$NumRings
train_data<-df_abl[,!names(df_abl) %in% "NumRings"]
# str(train_data)
# Sex is categorical data
# str(train_data$Sex)
# any(is.na(train_data$Sex))
```


### 2.2 / Encoding Categorical Variables <br>
Because the sex category is not an ordinal relationship(ex. small, medium large), 
dimension is not a big concern since there are only 8. So, I chose one-hot 
encoding for the Sex Categorical data. One-Hot is better for nominal categories 
and linear models since it interprets the encoded variables as independent 
features instead of continuous numbers like 1, 2, and 3, as the label encoding 
method used.

```{r encoding, message=FALSE, warning=FALSE}
#One Hot Encoding Sex column
encoded_sex <- model.matrix(~Sex - 1, data=train_data)

# Exclude Sex, Bind encoded_sex columns to original dataframe
train_data <- cbind(train_data[, !(names(train_data) %in% "Sex")], encoded_sex)
```

### 2.3 / Normalize numerical values in train data
```{r normalize_trainset,message=FALSE, warning=FALSE}
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x))) }

# Normalize all columns
train_data <- as.data.frame(lapply(train_data, normalize))

```


### 2.4 / Regression KNN 
```{r write_RegKNN,message=FALSE, warning=FALSE}
# The new_data are new abalone samples (one row per aba sample), so we need to 
# create an empty vector to store each prediction as output of the knn.reg.

knn.reg <- function(new_data, target_data, train_data, k) {

  # Convert dataframe to matrix for calculating distance (row vs point) 
  new_data_mat <- as.matrix(new_data)
  train_data_mat <- as.matrix(train_data)
  
  predictions <- rep(0,nrow(new_data))

  for (i in 1:nrow(new_data_mat)) {
  # Euclidean distance between train_data row and new_data point
  distances <- sqrt(rowSums((train_data_mat - new_data_mat[i,])^2))

  # Sort distance and select k nearest neighbors
  k_neighbors <- target_data[order(distances)[1:k]]

  # Weight K nearest neighbors, the nearest *2, second nearest *1.5, rest *1
  weights <- c(2, 1.5, rep(1, k-2))

  # Make k nearest neighbors in weighted average
  k_neighbors_avg <- round(sum(k_neighbors * weights) / sum(weights),2)

  # Store the avg value in prediction
  predictions[i] <- k_neighbors_avg
  }
  # Return predictions
  return(predictions)
}

```

### 2.5 / Forecast 
```{r forecast_test,message=FALSE, warning=FALSE}
new_data <- data.frame(
  Length = 0.82,
  Diameter = 0.491,
  Height = 0.361,
  ShuckedWeight = 0.3245,
  VisceraWeight = 0.0921,
  ShellWeight = 0.305,
  WholeWeight = 0.5538,
  Sex = "F" 
)

# Encoding
new_data$SexF <- as.integer(new_data$Sex == "F")
new_data$SexI <- as.integer(new_data$Sex == "I")
new_data$SexM <- as.integer(new_data$Sex == "M")

# Exclude Sex 
new_data <- new_data[, !(names(new_data) %in% "Sex")]
# Check does all columns match with train

# Run knn.reg
ring_predictions <- knn.reg(new_data,target_data,train_data,3)

```
<b> Sample Forcast Result </b> <br>
``Sex: F | Length: 0.82 | Diameter: 0.491 | Height: 0.361 | Whole weight: 0.5538``
``| Shucked weight: 0.3245 | Viscera weight: 0.0921 | Shell weight: 0.305`` <br>
**The ring number of this sample forecast is `r ring_predictions`**
<br>

### 2.6 /  Root Mean Squared Error (RMSE)

```{r RMSE,message=FALSE, warning=FALSE}
rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}

# One-Hot Encoding
encodeding <- model.matrix(~Sex - 1, data=df_abl)
df_abl2 <- cbind(df_abl[, !(names(df_abl) %in% "Sex")], encodeding)
# any(is.na(df_abl2))

# Random sampling 20% of the data as test data
set.seed(125)
rdsample <- sample(nrow(df_abl2), size = round(0.2 * nrow(df_abl2)), replace = FALSE)

# Split into train and test
test_rd  <- df_abl2[rdsample,]
train_rd <- df_abl2[-rdsample,]

# Extract target data
target_test_rd<- test_rd$NumRings
target_train_rd<- train_rd$NumRings


# Remove target data from train and test
train_rd <- test_rd[, !(names(test_rd) %in% "NumRings")]
test_rd <- test_rd[, !(names(test_rd) %in% "NumRings")]

# Normalization
train_rd <- as.data.frame(lapply(train_rd, normalize))
test_rd <- as.data.frame(lapply(test_rd, normalize))

# Prediction
predictions <- knn.reg(test_rd, target_train_rd, train_rd, k = 3)

# Evaluation
rmse <- rmse(target_test_rd,predictions)
```
**The Root Mean Squared Error (RMSE) is `r rmse`**
<br>
<br>
<br>

## 3 / Forecasting Future Sales Price
```{r forecast_MA,echo=FALSE, message=FALSE, warning=FALSE,results='asis'}
# Load dataframe
df_sale<-read.csv("property-sales.csv")
# head(df_sale)
# tail(df_sale)
# str(df_sale)
# any(is.na(df_sale))

# Calculate sale transactions
# nrow(df_sale)

# Parse datasold column to years
dates<-as.Date(df_sale$datesold, format = "%m/%d/%y")
df_sale$years<-format(dates, "%Y")
# min(df_sale$years)
# max(df_sale$years)

# Mean, SD of price
# mean(df_sale$price)
# sd(df_sale$price)
```
We obtained a data set containing `r nrow(df_sale)` sales transactions for the years `r min(df_sale$years)` to `r max(df_sale$years)`. The mean sales price for the entire time frame was `r round(mean(df_sale$price))` (sd = `r round(sd(df_sale$price))`).


Broken down by year, we have the following average sales prices per year:
```{r aggregate_table, echo=FALSE, message=FALSE, warning=FALSE}
# Aggregate price, years
avg_price_year <- aggregate(price ~ years, data = df_sale, mean)
knitr::kable(avg_price_year, format = "html",align = "l") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```


As the graph below shows, the average sales price per year has been increasing.
```{r linegraph,echo=FALSE, message=FALSE, warning=FALSE,results='asis'}
# Remane columns in table
names(avg_price_year) <- c("Year", "Average Sales Price")

# Plot line graph
ggplot(avg_price_year, aes(x = as.numeric(avg_price_year$Year), y = avg_price_year$`Average Sales Price`)) +
  geom_line(color = "red") +
  labs(x = "Year", y = "Average Sales Price", title = "") +
  theme_minimal()
```


```{r MA_calculation,echo=FALSE,message=FALSE, warning=FALSE,results='asis'}
# Use prior 3 years weighted moving average to forecast next year
w <- c(1,3,4)

next_year_avg<-sum(tail(avg_price_year$`Average Sales Price`,3)*w)/sum(w) 
```

Using a weighted moving average forecasting model that averages the prior 3 years (with weights of 4, 3, and 1), we predict next year's average sales price to be around $`r next_year_avg`.
